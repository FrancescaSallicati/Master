{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Third Assignment__ Borja Ruiz Amantegui, Francesca Sallicati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SOME PRELIMINARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fra12\\\\Desktop\\\\Francesca\\\\UC3M\\\\Big Data Intelligence'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some libraries\n",
    "import matplotlib.pyplot as plt \n",
    "# For plotting data\n",
    "import numpy as np              \n",
    "# For Panda dataframes. A dataframe is a matrix-like structure, \n",
    "# similar to R dataframes  \n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The \"wind_pickle\" file contains data in a binary format called \"Pickle\". Pickle data loads faster than text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the attributes in the dataset. Very important, the output attribute (i.e. the value to be predicted, **energy**, is the first attribute). **Steps** represents the hours in advance of the forecast. We will not use this variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5937, 556)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['energy',\n",
       " 'steps',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'p54.162.1',\n",
       " 'p54.162.2',\n",
       " 'p54.162.3',\n",
       " 'p54.162.4',\n",
       " 'p54.162.5',\n",
       " 'p54.162.6',\n",
       " 'p54.162.7',\n",
       " 'p54.162.8',\n",
       " 'p54.162.9',\n",
       " 'p54.162.10',\n",
       " 'p54.162.11',\n",
       " 'p54.162.12',\n",
       " 'p54.162.13',\n",
       " 'p54.162.14',\n",
       " 'p54.162.15',\n",
       " 'p54.162.16',\n",
       " 'p54.162.17',\n",
       " 'p54.162.18',\n",
       " 'p54.162.19',\n",
       " 'p54.162.20',\n",
       " 'p54.162.21',\n",
       " 'p54.162.22',\n",
       " 'p54.162.23',\n",
       " 'p54.162.24',\n",
       " 'p54.162.25',\n",
       " 'p55.162.1',\n",
       " 'p55.162.2',\n",
       " 'p55.162.3',\n",
       " 'p55.162.4',\n",
       " 'p55.162.5',\n",
       " 'p55.162.6',\n",
       " 'p55.162.7',\n",
       " 'p55.162.8',\n",
       " 'p55.162.9',\n",
       " 'p55.162.10',\n",
       " 'p55.162.11',\n",
       " 'p55.162.12',\n",
       " 'p55.162.13',\n",
       " 'p55.162.14',\n",
       " 'p55.162.15',\n",
       " 'p55.162.16',\n",
       " 'p55.162.17',\n",
       " 'p55.162.18',\n",
       " 'p55.162.19',\n",
       " 'p55.162.20',\n",
       " 'p55.162.21',\n",
       " 'p55.162.22',\n",
       " 'p55.162.23',\n",
       " 'p55.162.24',\n",
       " 'p55.162.25',\n",
       " 'cape.1',\n",
       " 'cape.2',\n",
       " 'cape.3',\n",
       " 'cape.4',\n",
       " 'cape.5',\n",
       " 'cape.6',\n",
       " 'cape.7',\n",
       " 'cape.8',\n",
       " 'cape.9',\n",
       " 'cape.10',\n",
       " 'cape.11',\n",
       " 'cape.12',\n",
       " 'cape.13',\n",
       " 'cape.14',\n",
       " 'cape.15',\n",
       " 'cape.16',\n",
       " 'cape.17',\n",
       " 'cape.18',\n",
       " 'cape.19',\n",
       " 'cape.20',\n",
       " 'cape.21',\n",
       " 'cape.22',\n",
       " 'cape.23',\n",
       " 'cape.24',\n",
       " 'cape.25',\n",
       " 'p59.162.1',\n",
       " 'p59.162.2',\n",
       " 'p59.162.3',\n",
       " 'p59.162.4',\n",
       " 'p59.162.5',\n",
       " 'p59.162.6',\n",
       " 'p59.162.7',\n",
       " 'p59.162.8',\n",
       " 'p59.162.9',\n",
       " 'p59.162.10',\n",
       " 'p59.162.11',\n",
       " 'p59.162.12',\n",
       " 'p59.162.13',\n",
       " 'p59.162.14',\n",
       " 'p59.162.15',\n",
       " 'p59.162.16',\n",
       " 'p59.162.17',\n",
       " 'p59.162.18',\n",
       " 'p59.162.19',\n",
       " 'p59.162.20',\n",
       " 'p59.162.21',\n",
       " 'p59.162.22',\n",
       " 'p59.162.23',\n",
       " 'p59.162.24',\n",
       " 'p59.162.25',\n",
       " 'lai_lv.1',\n",
       " 'lai_lv.2',\n",
       " 'lai_lv.3',\n",
       " 'lai_lv.4',\n",
       " 'lai_lv.5',\n",
       " 'lai_lv.6',\n",
       " 'lai_lv.7',\n",
       " 'lai_lv.8',\n",
       " 'lai_lv.9',\n",
       " 'lai_lv.10',\n",
       " 'lai_lv.11',\n",
       " 'lai_lv.12',\n",
       " 'lai_lv.13',\n",
       " 'lai_lv.14',\n",
       " 'lai_lv.15',\n",
       " 'lai_lv.16',\n",
       " 'lai_lv.17',\n",
       " 'lai_lv.18',\n",
       " 'lai_lv.19',\n",
       " 'lai_lv.20',\n",
       " 'lai_lv.21',\n",
       " 'lai_lv.22',\n",
       " 'lai_lv.23',\n",
       " 'lai_lv.24',\n",
       " 'lai_lv.25',\n",
       " 'lai_hv.1',\n",
       " 'lai_hv.2',\n",
       " 'lai_hv.3',\n",
       " 'lai_hv.4',\n",
       " 'lai_hv.5',\n",
       " 'lai_hv.6',\n",
       " 'lai_hv.7',\n",
       " 'lai_hv.8',\n",
       " 'lai_hv.9',\n",
       " 'lai_hv.10',\n",
       " 'lai_hv.11',\n",
       " 'lai_hv.12',\n",
       " 'lai_hv.13',\n",
       " 'lai_hv.14',\n",
       " 'lai_hv.15',\n",
       " 'lai_hv.16',\n",
       " 'lai_hv.17',\n",
       " 'lai_hv.18',\n",
       " 'lai_hv.19',\n",
       " 'lai_hv.20',\n",
       " 'lai_hv.21',\n",
       " 'lai_hv.22',\n",
       " 'lai_hv.23',\n",
       " 'lai_hv.24',\n",
       " 'lai_hv.25',\n",
       " 'u10n.1',\n",
       " 'u10n.2',\n",
       " 'u10n.3',\n",
       " 'u10n.4',\n",
       " 'u10n.5',\n",
       " 'u10n.6',\n",
       " 'u10n.7',\n",
       " 'u10n.8',\n",
       " 'u10n.9',\n",
       " 'u10n.10',\n",
       " 'u10n.11',\n",
       " 'u10n.12',\n",
       " 'u10n.13',\n",
       " 'u10n.14',\n",
       " 'u10n.15',\n",
       " 'u10n.16',\n",
       " 'u10n.17',\n",
       " 'u10n.18',\n",
       " 'u10n.19',\n",
       " 'u10n.20',\n",
       " 'u10n.21',\n",
       " 'u10n.22',\n",
       " 'u10n.23',\n",
       " 'u10n.24',\n",
       " 'u10n.25',\n",
       " 'v10n.1',\n",
       " 'v10n.2',\n",
       " 'v10n.3',\n",
       " 'v10n.4',\n",
       " 'v10n.5',\n",
       " 'v10n.6',\n",
       " 'v10n.7',\n",
       " 'v10n.8',\n",
       " 'v10n.9',\n",
       " 'v10n.10',\n",
       " 'v10n.11',\n",
       " 'v10n.12',\n",
       " 'v10n.13',\n",
       " 'v10n.14',\n",
       " 'v10n.15',\n",
       " 'v10n.16',\n",
       " 'v10n.17',\n",
       " 'v10n.18',\n",
       " 'v10n.19',\n",
       " 'v10n.20',\n",
       " 'v10n.21',\n",
       " 'v10n.22',\n",
       " 'v10n.23',\n",
       " 'v10n.24',\n",
       " 'v10n.25',\n",
       " 'sp.1',\n",
       " 'sp.2',\n",
       " 'sp.3',\n",
       " 'sp.4',\n",
       " 'sp.5',\n",
       " 'sp.6',\n",
       " 'sp.7',\n",
       " 'sp.8',\n",
       " 'sp.9',\n",
       " 'sp.10',\n",
       " 'sp.11',\n",
       " 'sp.12',\n",
       " 'sp.13',\n",
       " 'sp.14',\n",
       " 'sp.15',\n",
       " 'sp.16',\n",
       " 'sp.17',\n",
       " 'sp.18',\n",
       " 'sp.19',\n",
       " 'sp.20',\n",
       " 'sp.21',\n",
       " 'sp.22',\n",
       " 'sp.23',\n",
       " 'sp.24',\n",
       " 'sp.25',\n",
       " 'stl1.1',\n",
       " 'stl1.2',\n",
       " 'stl1.3',\n",
       " 'stl1.4',\n",
       " 'stl1.5',\n",
       " 'stl1.6',\n",
       " 'stl1.7',\n",
       " 'stl1.8',\n",
       " 'stl1.9',\n",
       " 'stl1.10',\n",
       " 'stl1.11',\n",
       " 'stl1.12',\n",
       " 'stl1.13',\n",
       " 'stl1.14',\n",
       " 'stl1.15',\n",
       " 'stl1.16',\n",
       " 'stl1.17',\n",
       " 'stl1.18',\n",
       " 'stl1.19',\n",
       " 'stl1.20',\n",
       " 'stl1.21',\n",
       " 'stl1.22',\n",
       " 'stl1.23',\n",
       " 'stl1.24',\n",
       " 'stl1.25',\n",
       " 'u10.1',\n",
       " 'u10.2',\n",
       " 'u10.3',\n",
       " 'u10.4',\n",
       " 'u10.5',\n",
       " 'u10.6',\n",
       " 'u10.7',\n",
       " 'u10.8',\n",
       " 'u10.9',\n",
       " 'u10.10',\n",
       " 'u10.11',\n",
       " 'u10.12',\n",
       " 'u10.13',\n",
       " 'u10.14',\n",
       " 'u10.15',\n",
       " 'u10.16',\n",
       " 'u10.17',\n",
       " 'u10.18',\n",
       " 'u10.19',\n",
       " 'u10.20',\n",
       " 'u10.21',\n",
       " 'u10.22',\n",
       " 'u10.23',\n",
       " 'u10.24',\n",
       " 'u10.25',\n",
       " 'v10.1',\n",
       " 'v10.2',\n",
       " 'v10.3',\n",
       " 'v10.4',\n",
       " 'v10.5',\n",
       " 'v10.6',\n",
       " 'v10.7',\n",
       " 'v10.8',\n",
       " 'v10.9',\n",
       " 'v10.10',\n",
       " 'v10.11',\n",
       " 'v10.12',\n",
       " 'v10.13',\n",
       " 'v10.14',\n",
       " 'v10.15',\n",
       " 'v10.16',\n",
       " 'v10.17',\n",
       " 'v10.18',\n",
       " 'v10.19',\n",
       " 'v10.20',\n",
       " 'v10.21',\n",
       " 'v10.22',\n",
       " 'v10.23',\n",
       " 'v10.24',\n",
       " 'v10.25',\n",
       " 't2m.1',\n",
       " 't2m.2',\n",
       " 't2m.3',\n",
       " 't2m.4',\n",
       " 't2m.5',\n",
       " 't2m.6',\n",
       " 't2m.7',\n",
       " 't2m.8',\n",
       " 't2m.9',\n",
       " 't2m.10',\n",
       " 't2m.11',\n",
       " 't2m.12',\n",
       " 't2m.13',\n",
       " 't2m.14',\n",
       " 't2m.15',\n",
       " 't2m.16',\n",
       " 't2m.17',\n",
       " 't2m.18',\n",
       " 't2m.19',\n",
       " 't2m.20',\n",
       " 't2m.21',\n",
       " 't2m.22',\n",
       " 't2m.23',\n",
       " 't2m.24',\n",
       " 't2m.25',\n",
       " 'stl2.1',\n",
       " 'stl2.2',\n",
       " 'stl2.3',\n",
       " 'stl2.4',\n",
       " 'stl2.5',\n",
       " 'stl2.6',\n",
       " 'stl2.7',\n",
       " 'stl2.8',\n",
       " 'stl2.9',\n",
       " 'stl2.10',\n",
       " 'stl2.11',\n",
       " 'stl2.12',\n",
       " 'stl2.13',\n",
       " 'stl2.14',\n",
       " 'stl2.15',\n",
       " 'stl2.16',\n",
       " 'stl2.17',\n",
       " 'stl2.18',\n",
       " 'stl2.19',\n",
       " 'stl2.20',\n",
       " 'stl2.21',\n",
       " 'stl2.22',\n",
       " 'stl2.23',\n",
       " 'stl2.24',\n",
       " 'stl2.25',\n",
       " 'stl3.1',\n",
       " 'stl3.2',\n",
       " 'stl3.3',\n",
       " 'stl3.4',\n",
       " 'stl3.5',\n",
       " 'stl3.6',\n",
       " 'stl3.7',\n",
       " 'stl3.8',\n",
       " 'stl3.9',\n",
       " 'stl3.10',\n",
       " 'stl3.11',\n",
       " 'stl3.12',\n",
       " 'stl3.13',\n",
       " 'stl3.14',\n",
       " 'stl3.15',\n",
       " 'stl3.16',\n",
       " 'stl3.17',\n",
       " 'stl3.18',\n",
       " 'stl3.19',\n",
       " 'stl3.20',\n",
       " 'stl3.21',\n",
       " 'stl3.22',\n",
       " 'stl3.23',\n",
       " 'stl3.24',\n",
       " 'stl3.25',\n",
       " 'iews.1',\n",
       " 'iews.2',\n",
       " 'iews.3',\n",
       " 'iews.4',\n",
       " 'iews.5',\n",
       " 'iews.6',\n",
       " 'iews.7',\n",
       " 'iews.8',\n",
       " 'iews.9',\n",
       " 'iews.10',\n",
       " 'iews.11',\n",
       " 'iews.12',\n",
       " 'iews.13',\n",
       " 'iews.14',\n",
       " 'iews.15',\n",
       " 'iews.16',\n",
       " 'iews.17',\n",
       " 'iews.18',\n",
       " 'iews.19',\n",
       " 'iews.20',\n",
       " 'iews.21',\n",
       " 'iews.22',\n",
       " 'iews.23',\n",
       " 'iews.24',\n",
       " 'iews.25',\n",
       " 'inss.1',\n",
       " 'inss.2',\n",
       " 'inss.3',\n",
       " 'inss.4',\n",
       " 'inss.5',\n",
       " 'inss.6',\n",
       " 'inss.7',\n",
       " 'inss.8',\n",
       " 'inss.9',\n",
       " 'inss.10',\n",
       " 'inss.11',\n",
       " 'inss.12',\n",
       " 'inss.13',\n",
       " 'inss.14',\n",
       " 'inss.15',\n",
       " 'inss.16',\n",
       " 'inss.17',\n",
       " 'inss.18',\n",
       " 'inss.19',\n",
       " 'inss.20',\n",
       " 'inss.21',\n",
       " 'inss.22',\n",
       " 'inss.23',\n",
       " 'inss.24',\n",
       " 'inss.25',\n",
       " 'stl4.1',\n",
       " 'stl4.2',\n",
       " 'stl4.3',\n",
       " 'stl4.4',\n",
       " 'stl4.5',\n",
       " 'stl4.6',\n",
       " 'stl4.7',\n",
       " 'stl4.8',\n",
       " 'stl4.9',\n",
       " 'stl4.10',\n",
       " 'stl4.11',\n",
       " 'stl4.12',\n",
       " 'stl4.13',\n",
       " 'stl4.14',\n",
       " 'stl4.15',\n",
       " 'stl4.16',\n",
       " 'stl4.17',\n",
       " 'stl4.18',\n",
       " 'stl4.19',\n",
       " 'stl4.20',\n",
       " 'stl4.21',\n",
       " 'stl4.22',\n",
       " 'stl4.23',\n",
       " 'stl4.24',\n",
       " 'stl4.25',\n",
       " 'fsr.1',\n",
       " 'fsr.2',\n",
       " 'fsr.3',\n",
       " 'fsr.4',\n",
       " 'fsr.5',\n",
       " 'fsr.6',\n",
       " 'fsr.7',\n",
       " 'fsr.8',\n",
       " 'fsr.9',\n",
       " 'fsr.10',\n",
       " 'fsr.11',\n",
       " 'fsr.12',\n",
       " 'fsr.13',\n",
       " 'fsr.14',\n",
       " 'fsr.15',\n",
       " 'fsr.16',\n",
       " 'fsr.17',\n",
       " 'fsr.18',\n",
       " 'fsr.19',\n",
       " 'fsr.20',\n",
       " 'fsr.21',\n",
       " 'fsr.22',\n",
       " 'fsr.23',\n",
       " 'fsr.24',\n",
       " 'fsr.25',\n",
       " 'flsr.1',\n",
       " 'flsr.2',\n",
       " 'flsr.3',\n",
       " 'flsr.4',\n",
       " 'flsr.5',\n",
       " 'flsr.6',\n",
       " 'flsr.7',\n",
       " 'flsr.8',\n",
       " 'flsr.9',\n",
       " 'flsr.10',\n",
       " 'flsr.11',\n",
       " 'flsr.12',\n",
       " 'flsr.13',\n",
       " 'flsr.14',\n",
       " 'flsr.15',\n",
       " 'flsr.16',\n",
       " 'flsr.17',\n",
       " 'flsr.18',\n",
       " 'flsr.19',\n",
       " 'flsr.20',\n",
       " 'flsr.21',\n",
       " 'flsr.22',\n",
       " 'flsr.23',\n",
       " 'flsr.24',\n",
       " 'flsr.25',\n",
       " 'u100.1',\n",
       " 'u100.2',\n",
       " 'u100.3',\n",
       " 'u100.4',\n",
       " 'u100.5',\n",
       " 'u100.6',\n",
       " 'u100.7',\n",
       " 'u100.8',\n",
       " 'u100.9',\n",
       " 'u100.10',\n",
       " 'u100.11',\n",
       " 'u100.12',\n",
       " 'u100.13',\n",
       " 'u100.14',\n",
       " 'u100.15',\n",
       " 'u100.16',\n",
       " 'u100.17',\n",
       " 'u100.18',\n",
       " 'u100.19',\n",
       " 'u100.20',\n",
       " 'u100.21',\n",
       " 'u100.22',\n",
       " 'u100.23',\n",
       " 'u100.24',\n",
       " 'u100.25',\n",
       " 'v100.1',\n",
       " 'v100.2',\n",
       " 'v100.3',\n",
       " 'v100.4',\n",
       " 'v100.5',\n",
       " 'v100.6',\n",
       " 'v100.7',\n",
       " 'v100.8',\n",
       " 'v100.9',\n",
       " 'v100.10',\n",
       " 'v100.11',\n",
       " 'v100.12',\n",
       " 'v100.13',\n",
       " 'v100.14',\n",
       " 'v100.15',\n",
       " 'v100.16',\n",
       " 'v100.17',\n",
       " 'v100.18',\n",
       " 'v100.19',\n",
       " 'v100.20',\n",
       " 'v100.21',\n",
       " 'v100.22',\n",
       " 'v100.23',\n",
       " 'v100.24',\n",
       " 'v100.25']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset contains 5937 instances and 556 attributes (including \n",
    "# the outcome to be predicted)\n",
    "print data.shape\n",
    "data.columns.values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'steps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values.tolist()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below, data is going to be separated in train, validation, and test. Given that the use of Pandas dataframes is quite advanced, I am doing this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2528L,)\n",
      "(1299L,)\n",
      "(2110L,)\n"
     ]
    }
   ],
   "source": [
    "indicesTrain = (np.where(data.year<=2006))[0]\n",
    "print(indicesTrain.shape)\n",
    "indicesVal = (np.where((data.year==2007) | (data.year==2008)))[0]\n",
    "print(indicesVal.shape)\n",
    "indicesTest = (np.where(data.year>=2009))[0]\n",
    "print(indicesTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware!, **indicesTrain** does not contain the training data, but the *indices* of the training data. For instance, the following cell means that training data is made of instance number 0, instance number 1, ..., up to instance number 2527. This will be important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2528L,)\n"
     ]
    }
   ],
   "source": [
    "indicesTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to transform **data**, which is a Pandas dataframe, to **ava**, which is a NumPy matrix. The reason is that Scikit-learn uses NumPy matrices, not Panda dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ava = data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **ava** is going to be decomposed into inputs **X** and outputs **y**. And then, into training, validation, and test. For instance, **Xava** and **yava** contain the input attributes, and the output attribute (**energy**) of the whole dataset. Please, ask yourself why the inputs use \"6:\" and the output use \"0\". **Xtrain** and **ytrain** are the same, but for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xava = ava[:,6:]; yava = ava[:,0]\n",
    "Xtrain = ava[indicesTrain,6:]; ytrain = ava[indicesTrain,0]\n",
    "Xval = ava[indicesVal,6:]; yval = ava[indicesVal,0]\n",
    "Xtest = ava[indicesTest,6:]; ytest = ava[indicesTest,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines function **mae** (Mean Absolute Error), that we will use later to measure the accuracy of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mae(yval_pred, yval):\n",
    "  val_mae = metrics.mean_absolute_error(yval_pred, yval)\n",
    "  return(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains KNN with (Xtrain, ytrain) and evaluates it with (Xval, yval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43 ms\n",
      "MAE for KNN with K=5 is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "np.random.seed(0)\n",
    "%time _ = knn.fit(Xtrain, ytrain)\n",
    "yval_pred = knn.predict(Xval)\n",
    "\n",
    "print \"MAE for KNN with K=5 is {}\".format(mae(yval_pred, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsRegressor in sklearn.neighbors:\n",
      "\n",
      "sklearn.neighbors.KNeighborsRegressor = class KNeighborsRegressor(sklearn.neighbors.base.NeighborsBase, sklearn.neighbors.base.KNeighborsMixin, sklearn.neighbors.base.SupervisedFloatMixin, sklearn.base.RegressorMixin)\n",
      " |  Regression based on k-nearest neighbors.\n",
      " |  \n",
      " |  The target is predicted by local interpolation of the targets\n",
      " |  associated of the nearest neighbors in the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, optional (default = 5)\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : str or callable\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |      Uniform weights are used by default.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, optional (default = 30)\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : integer, optional (default = 2)\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : string or callable, default 'minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of the DistanceMetric class for a\n",
      " |      list of available metrics.\n",
      " |  \n",
      " |  metric_params : dict, optional (default = None)\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, optional (default = 1)\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      " |  >>> neigh.fit(X, y) # doctest: +ELLIPSIS\n",
      " |  KNeighborsRegressor(...)\n",
      " |  >>> print(neigh.predict([[1.5]]))\n",
      " |  [ 0.5]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  NearestNeighbors\n",
      " |  RadiusNeighborsRegressor\n",
      " |  KNeighborsClassifier\n",
      " |  RadiusNeighborsClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      " |     different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsRegressor\n",
      " |      sklearn.neighbors.base.NeighborsBase\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors.base.KNeighborsMixin\n",
      " |      sklearn.neighbors.base.SupervisedFloatMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of int, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          Target values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      ind : array\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
      " |      (array([[ 0.5]]), array([[2]]...))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  1.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.SupervisedFloatMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model using X as training data and y as target values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix}\n",
      " |          Target values, array of float values, shape = [n_samples]\n",
      " |           or [n_samples, n_outputs]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In case you need help for KNN\n",
    "help('sklearn.neighbors.KNeighborsRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell, does hyper-parameter tuning for parameter K (n_neighbors), from 1 to 4 by 1. Please, notice that with **partitions = [(indicesTrain, indicesVal)]** we are telling **gridSearch** to use the training dataset for training the different models with the different parameters, and the validation dataset for testing. Notice that this is different to other notebooks, where crossvalidation was used for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n",
      "Wall time: 1.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_neighbors': range(1,4,1)}\n",
    "\n",
    "partitions = [(indicesTrain, indicesVal)]\n",
    "clf = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_absolute_error',\n",
    "                   cv=partitions , verbose=1)\n",
    "%time _ = clf.fit(Xava,yava)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the best K parameter and the MAE of the final model built with the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: {'n_neighbors': 3} and MAE for best K: 503.711691044\n"
     ]
    }
   ],
   "source": [
    "print \"Best K: {} and MAE for best K: {}\".format(clf.best_params_, -clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HOW LONG DOES IT TAKE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to have some estimation of how long your machine learning algorithm is going to take. In the next two cells, try to estimate how many seconds KNN (with K=3) does it take, with only **100 instances**. With 6000 instances, it will take approximately 60 times that number. You can use **%time** for timing, as in previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n",
      "MAE for KNN with K=3 is 367.4187\n",
      "Wall time: 245 ms\n",
      "MAE for KNN with K=3 is 349.253959912\n"
     ]
    }
   ],
   "source": [
    "#new data (100 instances)\n",
    "Xava2 = ava[:100,6:]; yava2 = ava[:100,0]\n",
    "Xtrain2 = ava[:100,6:]; ytrain2 = ava[:100,0]\n",
    "Xval2 = ava[:100,6:]; yval2 = ava[:100,0]\n",
    "Xtest2 = ava[:100,6:]; ytest2 = ava[:100,0]\n",
    "\n",
    "#regressor\n",
    "n_neighbors2 = 3\n",
    "knn2 = neighbors.KNeighborsRegressor(n_neighbors2, weights='uniform')\n",
    "np.random.seed(0)\n",
    "%time _ = knn2.fit(Xtrain2, ytrain2)\n",
    "yval_pred2 = knn2.predict(Xval2)\n",
    "\n",
    "print \"MAE for KNN with K=3 is {}\".format(mae(yval_pred2, yval2))\n",
    "\n",
    "#new data (6000 instances)\n",
    "Xava3 = ava[:6000,6:]; yava3 = ava[:6000,0]\n",
    "Xtrain3 = ava[:6000,6:]; ytrain3 = ava[:6000,0]\n",
    "Xval3 = ava[:6000,6:]; yval3 = ava[:6000,0]\n",
    "Xtest3 = ava[:6000,6:]; ytest3 = ava[:6000,0]\n",
    "\n",
    "n_neighbors3 = 3\n",
    "knn3 = neighbors.KNeighborsRegressor(n_neighbors3, weights='uniform')\n",
    "np.random.seed(0)\n",
    "%time _ = knn3.fit(Xtrain3, ytrain3)\n",
    "yval_pred3 = knn3.predict(Xval3)\n",
    "\n",
    "print \"MAE for KNN with K=3 is {}\".format(mae(yval_pred3, yval3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, do the same for Decision trees with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41 ms\n",
      "MAE for Tree (100 instances) is 0.0\n",
      "Wall time: 4.94 s\n",
      "MAE for Tree (6000 instances) is 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#DecisionTreeRegressor for 100 instances\n",
    "tree1 = tree.DecisionTreeRegressor()\n",
    "tree1.fit(Xtrain2, ytrain2)\n",
    "np.random.seed(0)\n",
    "%time _ = tree1.fit(Xtrain2, ytrain2)\n",
    "yval_predtree1 = tree1.predict(Xval2)\n",
    "print \"MAE for Tree (100 instances) is {}\".format(mae(yval_predtree1, yval2))\n",
    "\n",
    "#DecisionTreeRegressor for 6000 instances\n",
    "tree2 = tree.DecisionTreeRegressor()\n",
    "tree2.fit(Xtrain3, ytrain3)\n",
    "np.random.seed(0)\n",
    "%time _ = tree2.fit(Xtrain3, ytrain3)\n",
    "yval_predtree2 = tree2.predict(Xval3)\n",
    "\n",
    "print \"MAE for Tree (6000 instances) is {}\".format(mae(yval_predtree2, yval3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL SELECTION AND HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a KNN model with default hiper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65 ms\n",
      "MAE for KNN with defult hyper-parameters is 486.911414935\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "knn.fit(Xtrain, ytrain) \n",
    "\n",
    "%time _ = knn.fit(Xtrain, ytrain)\n",
    "yval_predknn = knn.predict(Xval)\n",
    "\n",
    "print \"MAE for KNN with defult hyper-parameters is {}\".format(mae(yval_predknn, yval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for KNN. Can you improve results? Note: if **gridSearch** takes too long, you can use **Randomized Search** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:   45.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.6 s\n",
      "MAE for KNN with K={'n_neighbors': 33} is 474.536480276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_neighbors': range(2,34,1)}\n",
    "\n",
    "knnGS = GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = knnGS.fit(Xtrain,ytrain)\n",
    "\n",
    "yval_predknnGS = knnGS.predict(Xval)\n",
    "print \"MAE for KNN with K={} is {}\".format(knnGS.best_params_,mae(yval_predknnGS, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   10.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.1 s\n",
      "MAE for KNN with K={'n_neighbors': 30} is 474.437641006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_neighbors': range(2,34,1)}\n",
    "\n",
    "knnRS = RandomizedSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = knnRS.fit(Xtrain,ytrain)\n",
    "\n",
    "yval_predknnRS = knnRS.predict(Xval)\n",
    "print \"MAE for KNN with K={} is {}\".format(knnRS.best_params_,mae(yval_predknnRS, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#slightly better than the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree for regression with default hiper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Tree with default hyper parameters is 378.398660508\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "t = tree.DecisionTreeRegressor()\n",
    "t.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_predt = t.predict(Xval)\n",
    "print \"MAE for Tree with default hyper parameters is {}\".format(mae(yval_predt, yval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# better than KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Decision trees. Can you improve results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 560 out of 560 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 10s\n",
      "<bound method GridSearchCV.fit of GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32], 'max_depth': [2, 4, 6, 8, 10, 12, 14]},\n",
      "       pre_dispatch='2*n_jobs', refit=True,\n",
      "       scoring='neg_mean_squared_error', verbose=1)>\n",
      "MAE for Tree using GridSearch is 313.327883468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'max_depth': range(2,16,2),\n",
    "              'min_samples_split': range(2,34,2)}\n",
    "\n",
    "neightGS = GridSearchCV(tree.DecisionTreeRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = neightGS.fit(Xtrain,ytrain)\n",
    "yval_predtGS = neightGS.predict(Xval)\n",
    "\n",
    "neightGS.best_params_\n",
    "print \"MAE for Tree using GridSearch is {}\".format(mae(yval_predtGS, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   33.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.2 s\n",
      "MAE for Tree using RandomizedSearch is 317.503935226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "np.random.seed(0)\n",
    "param_grid = {'max_depth': range(2,16,2),\n",
    "              'min_samples_split': range(2,34,2)}\n",
    "\n",
    "neightRS = RandomizedSearchCV(tree.DecisionTreeRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = neightRS.fit(Xtrain,ytrain)\n",
    "\n",
    "yval_predtRS = neightRS.predict(Xval)\n",
    "print \"MAE for Tree using RandomizedSearch is {}\".format(mae(yval_predtRS, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#improves from 371 to 317"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Random Forest (RF) with default parameters. A RF is an ensemble technique based on Decision Trees, but instead of training just a single decision tree, it trains many of them and then computes the average of the outputs. Please, bear in mind that a RF with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in sklearn.ensemble:\n",
      "\n",
      "sklearn.ensemble.RandomForestRegressor = class RandomForestRegressor(ForestRegressor)\n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and use averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_split : float,\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool, optional (default=False)\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_prediction_ : array of shape = [n_samples]\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>>\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      " |             max_features='auto', max_leaf_nodes=None,\n",
      " |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |             min_samples_leaf=1, min_samples_split=2,\n",
      " |             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      " |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(regr.feature_importances_)\n",
      " |  [ 0.17339552  0.81594114  0.          0.01066333]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-2.50699856]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('sklearn.ensemble.RandomForestRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for RandomForest with default hyper parameters is 293.140709007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_predrf = rf.predict(Xval)\n",
    "print \"MAE for RandomForest with default hyper parameters is {}\".format(mae(yval_predrf, yval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#better than tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Random Forests. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 36.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37min 16s\n",
      "<bound method RandomizedSearchCV.fit of RandomizedSearchCV(cv=5, error_score='raise',\n",
      "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
      "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
      "          param_distributions={'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          scoring='neg_mean_squared_error', verbose=1)>\n",
      "MAE for RandomForest with n_estimators (number of trees in the forest) {'n_estimators': 55} using RandomizedSearch is 278.317294002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import ensemble\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_estimators': range(1,100)}\n",
    "\n",
    "rfRS = RandomizedSearchCV(ensemble.RandomForestRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = rfRS.fTit(Xtrain,ytrain)\n",
    "\n",
    "yval_predrfRS = rfRS.predict(Xval)\n",
    "print \"MAE for RandomForest with n_estimators (number of trees in the forest) {} using RandomizedSearch is {}\".format(rfRS.best_params_,mae(yval_predrfRS, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 60.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 3min 41s\n",
      "MAE for RandomForest with n_estimators (number of trees in the forest) {'n_estimators': 161} using RandomizedSearch is 278.080563501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import ensemble\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_estimators': range(1,200,20)}\n",
    "\n",
    "rfRS2 = RandomizedSearchCV(ensemble.RandomForestRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = rfRS2.fit(Xtrain,ytrain)\n",
    "\n",
    "yval_predrfRS2 = rfRS2.predict(Xval)\n",
    "print \"MAE for RandomForest with n_estimators (number of trees in the forest) {} using RandomizedSearch is {}\".format(rfRS2.best_params_,mae(yval_predrfRS2, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#since the value doesn't really change, we choose to stick to the previous model since it's less complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Gradient Tree Boosting (GB) with default parameters. A GB is also an ensemble technique based on Decision Trees. In this case, the second decision tree tries to fix the mistakes of the first decision tree. The third decision tree tries to fix the mistakes of the first two decision trees. An so on.\n",
    "\n",
    "Please, bear in mind that a GB with default parameters involves training 100 trees. You can estimate by hand how long it is going to take, and if it is excessive, you can lower the number of decision trees in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingRegressor in sklearn.ensemble:\n",
      "\n",
      "sklearn.ensemble.GradientBoostingRegressor = class GradientBoostingRegressor(BaseGradientBoosting, sklearn.base.RegressorMixin)\n",
      " |  Gradient Boosting for regression.\n",
      " |  \n",
      " |  GB builds an additive model in a forward stage-wise fashion;\n",
      " |  it allows for the optimization of arbitrary differentiable loss functions.\n",
      " |  In each stage a regression tree is fit on the negative gradient of the\n",
      " |  given loss function.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      " |      loss function to be optimized. 'ls' refers to least squares\n",
      " |      regression. 'lad' (least absolute deviation) is a highly robust\n",
      " |      loss function solely based on order information of the input\n",
      " |      variables. 'huber' is a combination of the two. 'quantile'\n",
      " |      allows quantile regression (use `alpha` to specify the quantile).\n",
      " |  \n",
      " |  learning_rate : float, optional (default=0.1)\n",
      " |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int (default=100)\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  max_depth : integer, optional (default=3)\n",
      " |      maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"friedman_mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"friedman_mse\" for the mean squared error with improvement\n",
      " |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      " |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  subsample : float, optional (default=1.0)\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=None)\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_split : float,\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  alpha : float (default=0.9)\n",
      " |      The alpha-quantile of the huber loss function and the quantile\n",
      " |      loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      " |  \n",
      " |  init : BaseEstimator, None, optional (default=None)\n",
      " |      An estimator object that is used to compute the initial\n",
      " |      predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      " |      If None it uses ``loss.init_estimator``.\n",
      " |  \n",
      " |  verbose : int, default: 0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  warm_start : bool, default: False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  presort : bool or 'auto', optional (default='auto')\n",
      " |      Whether to presort the data to speed up the finding of best splits in\n",
      " |      fitting. Auto mode by default will use presorting on dense data and\n",
      " |      default to normal sorting on sparse data. Setting presort to true on\n",
      " |      sparse data will raise an error.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         optional parameter *presort*.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : array, shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_improvement_ : array, shape = [n_estimators]\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |  \n",
      " |  train_score_ : array, shape = [n_estimators]\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init : BaseEstimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, RandomForestRegressor\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingRegressor\n",
      " |      BaseGradientBoosting\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict regression target at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of array of shape = [n_samples]\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values (integers in classification, real numbers in\n",
      " |          regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, optional\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  n_features\n",
      " |      DEPRECATED: Attribute n_features was deprecated in version 0.19 and will be removed in 0.21.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('sklearn.ensemble.GradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for GradientBoosting with default hyper parameters is 280.211847132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(Xtrain, ytrain)\n",
    "\n",
    "yval_predgb = gb.predict(Xval)\n",
    "print \"MAE for GradientBoosting with default hyper parameters is {}\".format(mae(yval_predgb, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#slightly worse than random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyper-parameter tuning for Gradient Boosting. Their main hyper-parameter is **n_estimators**, which is the number of decision trees in the ensemble. Check some values around the default value (like, 50, 100, 150, ...). Please, bear in mind this is going to take time ... In case you want to use other hyper-parameters, please ask the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 9s\n",
      "<bound method RandomizedSearchCV.fit of RandomizedSearchCV(cv=5, error_score='raise',\n",
      "          estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False),\n",
      "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
      "          param_distributions={'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          scoring='neg_mean_squared_error', verbose=1)>\n",
      "MAE for GradientBoosting with n_estimators (number of trees in the forest) {'n_estimators': 56} using RandomizedSearch is 279.445039609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import ensemble\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_estimators': range(1,100)}\n",
    "\n",
    "gbRS = RandomizedSearchCV(ensemble.GradientBoostingRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = gbRS.fit(Xtrain,ytrain)\n",
    "\n",
    "yval_predgbRS = gbRS.predict(Xval)\n",
    "print \"MAE for GradientBoosting with n_estimators (number of trees in the forest) {} using RandomizedSearch is {}\".format(gbRS.best_params_,mae(yval_predgbRS, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 23s\n",
      "MAE for GradientBoosting with n_estimators (number of trees in the forest) {'n_estimators': 61} using RandomizedSearch is 279.621406567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import ensemble\n",
    "np.random.seed(0)\n",
    "param_grid = {'n_estimators': range(1,200,20)}\n",
    "\n",
    "gbRS2 = RandomizedSearchCV(ensemble.GradientBoostingRegressor(), \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error',\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = gbRS2.fit(Xtrain,ytrain)\n",
    "\n",
    "yval_predgbRS2 = gbRS2.predict(Xval)\n",
    "print \"MAE for GradientBoosting with n_estimators (number of trees in the forest) {} using RandomizedSearch is {}\".format(gbRS2.best_params_,mae(yval_predgbRS2, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest performs better than Gradient Boosting, even though the difference is very little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should know which model performs best, and what hyper-parameters to use. Please, evaluate that best performing model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for RandomForest with n_estimators (number of trees in the forest) {'n_estimators': 55} using RandomizedSearch is 286.974804136\n"
     ]
    }
   ],
   "source": [
    "#The winning model is RandomForest with 55 trees \n",
    "yval_predrfRS = rfRS.predict(Xtest)\n",
    "print \"MAE for RandomForest with n_estimators (number of trees in the forest) {} using RandomizedSearch is {}\".format(rfRS.best_params_,mae(yval_predrfRS, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the error is slightly increasingon the test set, but not in a significant way (previous value was 278.317294)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ATTRIBUTE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to answer the following questions: \n",
    "\n",
    "- Are all 550 input attributes actually necessary in order to get a good model? Is it possible to have an accurate model that uses fewer than 550 variables? How many? \n",
    "- Is it enough to use only the attributes for the actual Sotavento location? (13th location in the grid)\n",
    "\n",
    "In order to answer these questions, you should consider the following:\n",
    "\n",
    "1) Go through the \"Attribute Selection\" ipython notebook, and understand the main ideas about **SelectKBest** and **Pipeline**.\n",
    "\n",
    "2) Use **SelectKBest** and **Pipeline** (and whatever else you need) in order to find a subset of attributes that allows to build an accurate Decision Tree model. We are going to use here Decision Trees because they are faster (even if Random Forests or Gradient Boosting performed better in previous sections). Please, note that you cannot just copy/paste from the \"Attribute Selection\" notebook. You will have to think about how to use the main ideas from that notebook, and change whatever needs changing. \n",
    "\n",
    "3) Use the test dataset in order to compare between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, -753.53688132167588)\n",
      "(76, -749.21346421993064)\n",
      "(80, -748.6797017431162)\n",
      "(85, -744.68734135065006)\n",
      "(81, -744.20529412531153)\n",
      "(77, -743.96210333581507)\n",
      "(90, -741.34600458316152)\n",
      "(86, -740.07992904757566)\n",
      "(82, -738.78450270272265)\n",
      "(78, -737.7686470702447)\n",
      "(91, -736.63409172556464)\n",
      "(87, -734.52984054435512)\n",
      "(95, -734.14633693685619)\n",
      "(83, -732.44041951536553)\n",
      "(92, -730.98718014162387)\n",
      "(79, -730.66391484725671)\n",
      "(96, -729.45741826441508)\n",
      "(88, -728.06169297204133)\n",
      "(84, -725.17299793595714)\n",
      "(93, -724.41427914390317)\n",
      "(97, -723.84055863491824)\n",
      "(89, -720.67749767302939)\n",
      "(98, -717.31231235999337)\n",
      "(94, -716.93257578654323)\n",
      "(99, -709.88809312274475)\n",
      "(419, -323.84153823556153)\n",
      "(414, -323.46270103513757)\n",
      "(409, -322.96563672470188)\n",
      "(404, -322.27387127344815)\n",
      "(424, -318.86423433762695)\n",
      "(418, -306.59127915421993)\n",
      "(413, -306.20574065839429)\n",
      "(408, -305.69469387565596)\n",
      "(403, -304.99169188286589)\n",
      "(423, -302.26359167875682)\n",
      "(417, -289.38865342612735)\n",
      "(412, -288.99519640339025)\n",
      "(407, -288.48385899860318)\n",
      "(402, -287.78189115050338)\n",
      "(422, -285.72653408877693)\n",
      "(416, -272.29530779972669)\n",
      "(411, -271.9178788081125)\n",
      "(406, -271.41477946496127)\n",
      "(401, -270.72604667030043)\n",
      "(421, -269.32207419096767)\n",
      "(199, -258.77875287370591)\n",
      "(194, -258.19974957503683)\n",
      "(415, -255.40758525876345)\n",
      "(189, -255.15943080434141)\n",
      "(410, -255.04670330029433)\n",
      "(405, -254.56862273041668)\n",
      "(400, -253.90701758335004)\n",
      "(420, -253.12406463133718)\n",
      "(184, -251.31547283427744)\n",
      "(379, -248.79593124838203)\n",
      "(384, -248.47220657700612)\n",
      "(389, -248.20094457129184)\n",
      "(394, -247.97985815375458)\n",
      "(179, -246.36140577764624)\n",
      "(495, -245.73357546541712)\n",
      "(399, -244.45506497355282)\n",
      "(198, -244.20881064613232)\n",
      "(193, -243.29171366046563)\n",
      "(496, -242.60555748304145)\n",
      "(188, -240.51177780507035)\n",
      "(299, -238.19499379921703)\n",
      "(497, -237.1679714376788)\n",
      "(183, -237.00810202968941)\n",
      "(294, -236.98899480740954)\n",
      "(549, -235.74728930171358)\n",
      "(544, -235.46463251807208)\n",
      "(378, -235.14368323266612)\n",
      "(289, -234.93922348937207)\n",
      "(383, -234.75711981202988)\n",
      "(388, -234.43482726945422)\n",
      "(393, -234.17848445203472)\n",
      "(539, -233.96785392317642)\n",
      "(178, -232.51460624771724)\n",
      "(284, -232.32883647266408)\n",
      "(534, -232.07105120509516)\n",
      "(398, -231.03943791036193)\n",
      "(197, -230.63046572157302)\n",
      "(498, -229.63960983730641)\n",
      "(529, -229.60579092887383)\n",
      "(192, -229.41112773728489)\n",
      "(279, -228.93238899563096)\n",
      "(187, -226.86878082184816)\n",
      "(298, -226.70880786995033)\n",
      "(293, -225.25431951076303)\n",
      "(548, -225.03721186282471)\n",
      "(543, -224.41832048513473)\n",
      "(470, -223.75813431629246)\n",
      "(182, -223.68155719904053)\n",
      "(288, -223.30894291077971)\n",
      "(538, -222.97282271847928)\n",
      "(377, -221.46889956845885)\n",
      "(533, -221.14956718902181)\n",
      "(382, -221.00929497325112)\n",
      "(283, -220.84476986609405)\n",
      "(387, -220.64029655507602)\n",
      "(392, -220.33270871756673)\n",
      "(499, -220.32469518922559)\n",
      "(177, -219.59335697521763)\n",
      "(528, -218.78617034370976)\n",
      "(196, -218.02347030371581)\n",
      "(278, -217.65193737515341)\n",
      "(397, -217.59144952458746)\n",
      "(450, -217.31373779013128)\n",
      "(191, -216.53641448248609)\n",
      "(455, -216.3168114286288)\n",
      "(297, -215.88346632951044)\n",
      "(460, -215.59115560170309)\n",
      "(465, -215.04282244317739)\n",
      "(547, -214.92356847811212)\n",
      "(186, -214.21393737266695)\n",
      "(292, -214.20467218597196)\n",
      "(542, -214.00795057723249)\n",
      "(537, -212.60713541625489)\n",
      "(287, -212.35611178632752)\n",
      "(181, -211.30061550088772)\n",
      "(532, -210.84827794512572)\n",
      "(282, -210.02848020685821)\n",
      "(527, -208.56986663528608)\n",
      "(376, -207.80825455148366)\n",
      "(176, -207.57699655427098)\n",
      "(381, -207.27374050537256)\n",
      "(277, -207.0266999142095)\n",
      "(386, -206.8482129272368)\n",
      "(391, -206.50444153283266)\n",
      "(195, -206.35300978616078)\n",
      "(296, -205.72855217836823)\n",
      "(546, -205.40984038616745)\n",
      "(190, -204.62697588377875)\n",
      "(541, -204.21798247205598)\n",
      "(396, -204.16329021589542)\n",
      "(291, -203.83912785358996)\n",
      "(536, -202.85853524331878)\n",
      "(185, -202.49416315684607)\n",
      "(286, -202.08444347795373)\n",
      "(531, -201.14963766695084)\n",
      "(281, -199.88506152886382)\n",
      "(180, -199.82691247438785)\n",
      "(526, -198.94947251774715)\n",
      "(276, -197.05421719352438)\n",
      "(471, -196.6092107978948)\n",
      "(545, -196.46915661714078)\n",
      "(175, -196.42035626551012)\n",
      "(295, -196.22500499338594)\n",
      "(540, -195.02677648126362)\n",
      "(375, -194.21517920981603)\n",
      "(290, -194.15476148864724)\n",
      "(535, -193.70272719806442)\n",
      "(380, -193.60169292014453)\n",
      "(385, -193.11768709754745)\n",
      "(390, -192.72582807378288)\n",
      "(285, -192.48428886815918)\n",
      "(530, -192.04043810875291)\n",
      "(451, -191.66509084648843)\n",
      "(395, -190.79495049715482)\n",
      "(456, -190.76849210182149)\n",
      "(280, -190.3908807208083)\n",
      "(461, -190.1011280698435)\n",
      "(525, -189.90447598101022)\n",
      "(466, -189.59493156786795)\n",
      "(275, -187.71215842952927)\n",
      "(0, -183.61263652468)\n",
      "(1, -183.21699531273566)\n",
      "(2, -182.79946686256025)\n",
      "(3, -182.35667350218017)\n",
      "(5, -182.08192941936574)\n",
      "(4, -181.89155011108491)\n",
      "(6, -181.68547313299172)\n",
      "(7, -181.26291164789464)\n",
      "(10, -180.86548657631198)\n",
      "(8, -180.82162488159784)\n",
      "(11, -180.46845492736398)\n",
      "(9, -180.35609242849478)\n",
      "(12, -180.04593001774327)\n",
      "(15, -179.87647447246988)\n",
      "(13, -179.60237763769103)\n",
      "(16, -179.47733071425759)\n",
      "(14, -179.134903649064)\n",
      "(17, -179.05581380226764)\n",
      "(18, -178.60994911605525)\n",
      "(19, -178.14160786337499)\n",
      "(20, -177.9295975305707)\n",
      "(21, -177.53821332588936)\n",
      "(22, -177.12210015972977)\n",
      "(23, -176.68209633481459)\n",
      "(24, -176.2219620906651)\n",
      "(490, -170.31849900124715)\n",
      "(491, -168.21378944599761)\n",
      "(492, -164.45506448734105)\n",
      "(472, -164.10866340385556)\n",
      "(452, -160.76370717361928)\n",
      "(457, -159.98273699446932)\n",
      "(462, -159.40220971829211)\n",
      "(493, -159.21212129602063)\n",
      "(467, -158.95035847094258)\n",
      "(494, -152.7100169423812)\n",
      "(473, -131.34803350438253)\n",
      "(453, -129.43888903210677)\n",
      "(458, -128.78158294862914)\n",
      "(463, -128.29299883351766)\n",
      "(468, -127.92648881845895)\n",
      "(259, -124.27178097391275)\n",
      "(254, -124.16808970336533)\n",
      "(264, -124.1161516218837)\n",
      "(269, -123.79668173747787)\n",
      "(258, -123.73972858361579)\n",
      "(263, -123.71877293970536)\n",
      "(268, -123.53212613528645)\n",
      "(253, -123.50994851725468)\n",
      "(262, -123.16695335654623)\n",
      "(267, -123.08977372272768)\n",
      "(257, -123.07729778762818)\n",
      "(504, -123.03082092107603)\n",
      "(252, -122.73964180487)\n",
      "(266, -122.50532951680707)\n",
      "(261, -122.48809857427709)\n",
      "(256, -122.30776881144405)\n",
      "(274, -122.20151108309045)\n",
      "(273, -122.13664423852487)\n",
      "(272, -121.87812611170125)\n",
      "(251, -121.87779731380837)\n",
      "(265, -121.80306096956268)\n",
      "(503, -121.71760878057469)\n",
      "(260, -121.71028994889549)\n",
      "(271, -121.46124888394067)\n",
      "(255, -121.45057176179458)\n",
      "(509, -121.26740328626904)\n",
      "(250, -120.94558649855102)\n",
      "(270, -120.91267027841985)\n",
      "(502, -120.36799447872383)\n",
      "(508, -120.08310079078458)\n",
      "(159, -120.03815435291499)\n",
      "(154, -119.96848337536564)\n",
      "(164, -119.78646024872481)\n",
      "(514, -119.68906296258417)\n",
      "(158, -119.34482399475606)\n",
      "(169, -119.32933352182322)\n",
      "(163, -119.22718123707618)\n",
      "(153, -119.15439186851498)\n",
      "(501, -118.99266322872671)\n",
      "(168, -118.91292191245587)\n",
      "(507, -118.85467946214642)\n",
      "(513, -118.62200719647575)\n",
      "(157, -118.52864958842974)\n",
      "(162, -118.52222879655959)\n",
      "(167, -118.32344434608122)\n",
      "(519, -118.28003099208144)\n",
      "(152, -118.24400205164939)\n",
      "(161, -117.69898507559498)\n",
      "(156, -117.61629654131826)\n",
      "(500, -117.60174070056972)\n",
      "(166, -117.59876945922157)\n",
      "(506, -117.59327203951599)\n",
      "(174, -117.50211148672065)\n",
      "(512, -117.50129225817463)\n",
      "(518, -117.31862895396794)\n",
      "(173, -117.25300490974561)\n",
      "(151, -117.25182570946161)\n",
      "(172, -116.81562973650804)\n",
      "(160, -116.78615803925062)\n",
      "(165, -116.76276266096521)\n",
      "(155, -116.62799336083762)\n",
      "(511, -116.34342908607324)\n",
      "(505, -116.30985039248462)\n",
      "(517, -116.29702818729929)\n",
      "(171, -116.22318407743627)\n",
      "(150, -116.20166174243818)\n",
      "(524, -116.19371514397857)\n",
      "(170, -115.51013147151401)\n",
      "(523, -115.37605191512941)\n",
      "(516, -115.229960218146)\n",
      "(510, -115.15767422449031)\n",
      "(522, -114.48854844174808)\n",
      "(515, -114.13083710088121)\n",
      "(521, -113.54658237678424)\n",
      "(520, -112.5649046569374)\n",
      "(200, -112.55317670749288)\n",
      "(201, -112.55099163554654)\n",
      "(202, -112.51740239764287)\n",
      "(203, -112.45295055521656)\n",
      "(204, -112.35100597079457)\n",
      "(206, -110.6325298286762)\n",
      "(207, -110.62782814535842)\n",
      "(205, -110.60359186120849)\n",
      "(208, -110.58869481082607)\n",
      "(209, -110.51790989169989)\n",
      "(212, -109.10964621714444)\n",
      "(213, -109.09517946342208)\n",
      "(211, -109.09351262554171)\n",
      "(214, -109.04935707356842)\n",
      "(210, -109.04025309889276)\n",
      "(218, -107.8716556343053)\n",
      "(217, -107.86872180903646)\n",
      "(219, -107.84008319082656)\n",
      "(216, -107.82955234940411)\n",
      "(215, -107.75913695645927)\n",
      "(229, -107.4650042279589)\n",
      "(234, -106.91349279929942)\n",
      "(228, -106.89525445096257)\n",
      "(239, -106.47734096279088)\n",
      "(249, -106.35395125892965)\n",
      "(233, -106.3433371938296)\n",
      "(227, -106.28156226702104)\n",
      "(244, -106.12579461048776)\n",
      "(238, -105.90702317391509)\n",
      "(248, -105.74865955217076)\n",
      "(232, -105.7296450730705)\n",
      "(226, -105.62248337992158)\n",
      "(243, -105.5558469893203)\n",
      "(223, -105.54490258137123)\n",
      "(224, -105.52901576167162)\n",
      "(222, -105.52372759656372)\n",
      "(221, -105.4672702641852)\n",
      "(220, -105.37877866966953)\n",
      "(237, -105.29294432786754)\n",
      "(247, -105.09903479736602)\n",
      "(231, -105.06789814689769)\n",
      "(242, -104.94013663419692)\n",
      "(225, -104.91522298582819)\n",
      "(236, -104.63115832846164)\n",
      "(246, -104.40082152233211)\n",
      "(230, -104.36012757427098)\n",
      "(241, -104.27773533376796)\n",
      "(235, -103.92052261092307)\n",
      "(245, -103.65349157543105)\n",
      "(149, -103.63757707366815)\n",
      "(134, -103.59622890569486)\n",
      "(139, -103.59617649249709)\n",
      "(129, -103.5958736988275)\n",
      "(144, -103.59525713199749)\n",
      "(240, -103.56523364982644)\n",
      "(148, -103.3254304043349)\n",
      "(138, -103.27463891671162)\n",
      "(128, -103.27423400340814)\n",
      "(143, -103.27405316751955)\n",
      "(133, -103.27398823155332)\n",
      "(147, -102.91236188047104)\n",
      "(127, -102.84955870729131)\n",
      "(132, -102.84858788545206)\n",
      "(142, -102.84827889442109)\n",
      "(137, -102.84712314240845)\n",
      "(146, -102.34263622116667)\n",
      "(136, -102.2593576008727)\n",
      "(141, -102.25892871090062)\n",
      "(126, -102.25802828476797)\n",
      "(131, -102.2569241443087)\n",
      "(474, -101.91822946625439)\n",
      "(145, -101.50597970820674)\n",
      "(125, -101.3856285501073)\n",
      "(140, -101.38504498563766)\n",
      "(135, -101.38447475231523)\n",
      "(130, -101.38436606252846)\n",
      "(454, -101.16812975127786)\n",
      "(124, -100.65790197429693)\n",
      "(459, -100.62329057604762)\n",
      "(119, -100.53434664357852)\n",
      "(109, -100.53392857680464)\n",
      "(104, -100.53376801682114)\n",
      "(114, -100.53329396694066)\n",
      "(329, -100.38133072677209)\n",
      "(464, -100.23262726635137)\n",
      "(334, -100.10204196505534)\n",
      "(469, -99.930317877934684)\n",
      "(123, -99.920578351519779)\n",
      "(349, -99.899470637627971)\n",
      "(339, -99.872707572709771)\n",
      "(108, -99.772762777356192)\n",
      "(103, -99.772284112921355)\n",
      "(118, -99.772252067322484)\n",
      "(113, -99.77202188553531)\n",
      "(328, -99.715998838010464)\n",
      "(344, -99.684443594307638)\n",
      "(333, -99.434332282447528)\n",
      "(122, -99.211030957620622)\n",
      "(348, -99.205615571772356)\n",
      "(338, -99.203092558300995)\n",
      "(112, -99.038092437967691)\n",
      "(107, -99.038067223662893)\n",
      "(102, -99.037808267499059)\n",
      "(117, -99.037756379580429)\n",
      "(327, -99.023634963650565)\n",
      "(343, -99.011855599304866)\n",
      "(332, -98.736064807791422)\n",
      "(121, -98.529380866185406)\n",
      "(337, -98.502080648567613)\n",
      "(347, -98.481191129159768)\n",
      "(111, -98.330492339763353)\n",
      "(101, -98.330290126633003)\n",
      "(116, -98.330138823235416)\n",
      "(106, -98.329760739164669)\n",
      "(342, -98.308869036432085)\n",
      "(326, -98.299567253473711)\n",
      "(331, -98.008478332495244)\n",
      "(120, -97.873095595413005)\n",
      "(336, -97.771482226805233)\n",
      "(346, -97.725934367060987)\n",
      "(115, -97.647810457072566)\n",
      "(110, -97.647799229965145)\n",
      "(100, -97.647581405465644)\n",
      "(105, -97.647096409766206)\n",
      "(341, -97.575107150756139)\n",
      "(325, -97.546427229914343)\n",
      "(330, -97.24927130897693)\n",
      "(335, -97.007696138823064)\n",
      "(345, -96.936519299617245)\n",
      "(340, -96.809304483743418)\n",
      "(354, -85.342684964826844)\n",
      "(359, -85.122518201247587)\n",
      "(353, -85.041600081441771)\n",
      "(374, -84.986431642273729)\n",
      "(364, -84.948534281763799)\n",
      "(358, -84.822219303681464)\n",
      "(369, -84.807612621819615)\n",
      "(352, -84.731043771642533)\n",
      "(373, -84.685360188845195)\n",
      "(363, -84.648852242825171)\n",
      "(357, -84.512551866212149)\n",
      "(368, -84.50870268211618)\n",
      "(351, -84.409176697126767)\n",
      "(372, -84.374712316829317)\n",
      "(362, -84.339732079291863)\n",
      "(367, -84.199544494073493)\n",
      "(356, -84.192020455144771)\n",
      "(350, -84.078356672854895)\n",
      "(371, -84.052811681732493)\n",
      "(361, -84.019269449612466)\n",
      "(366, -83.879286464609592)\n",
      "(355, -83.86063932085932)\n",
      "(370, -83.719953708997664)\n",
      "(360, -83.688117765756431)\n",
      "(365, -83.549226236478432)\n",
      "(324, -81.663339305018241)\n",
      "(304, -81.544975784316478)\n",
      "(309, -81.202495381827248)\n",
      "(314, -80.922028808092591)\n",
      "(319, -80.690201004073245)\n",
      "(303, -80.44776785518826)\n",
      "(323, -80.377893410902345)\n",
      "(308, -80.070301241064683)\n",
      "(313, -79.763661677314161)\n",
      "(318, -79.511831892015437)\n",
      "(302, -79.287157978755161)\n",
      "(322, -79.023593634642225)\n",
      "(307, -78.87305022525328)\n",
      "(312, -78.539619479185305)\n",
      "(317, -78.2670264351882)\n",
      "(301, -78.060356604316624)\n",
      "(306, -77.609179167171263)\n",
      "(321, -77.596790156548323)\n",
      "(311, -77.247474798551522)\n",
      "(316, -76.953144243449316)\n",
      "(300, -76.764842372706312)\n",
      "(305, -76.275186344080851)\n",
      "(320, -76.094306488748089)\n",
      "(310, -75.884487034337837)\n",
      "(315, -75.566254781622717)\n",
      "(485, -65.805906743792022)\n",
      "(486, -64.58893722895948)\n",
      "(487, -62.734523980328568)\n",
      "(488, -60.314667729302329)\n",
      "(489, -57.430339169191683)\n",
      "(479, -43.027979595642641)\n",
      "(478, -42.747465666323912)\n",
      "(477, -42.038808849706591)\n",
      "(476, -40.920700083208366)\n",
      "(475, -39.416384171496389)\n",
      "(425, -25.440467490130214)\n",
      "(426, -25.115871165849267)\n",
      "(427, -24.795124954759284)\n",
      "(428, -24.47797532362031)\n",
      "(429, -24.163830246359307)\n",
      "(430, -23.7957538721299)\n",
      "(431, -23.448543486558357)\n",
      "(432, -23.105441123235575)\n",
      "(433, -22.766075573396773)\n",
      "(435, -22.559461730751853)\n",
      "(434, -22.430679530875661)\n",
      "(436, -22.195970869471889)\n",
      "(437, -21.836598062331362)\n",
      "(440, -21.597640274618445)\n",
      "(438, -21.481752799585603)\n",
      "(445, -21.268623924617419)\n",
      "(441, -21.221712917662249)\n",
      "(439, -21.131115514290482)\n",
      "(446, -20.880759913194531)\n",
      "(442, -20.850597848604448)\n",
      "(447, -20.49840616402669)\n",
      "(443, -20.483996001838438)\n",
      "(444, -20.122345802522744)\n",
      "(448, -20.121252279569138)\n",
      "(449, -19.749062203831748)\n",
      "(69, -13.67051766802734)\n",
      "(74, -13.536290586909733)\n",
      "(64, -13.20397866724101)\n",
      "(59, -12.604423290511228)\n",
      "(68, -12.003279116327011)\n",
      "(73, -11.898214998411648)\n",
      "(54, -11.816400009028268)\n",
      "(63, -11.541828028093404)\n",
      "(58, -10.955506863463496)\n",
      "(67, -10.316454427221128)\n",
      "(72, -10.246822011136073)\n",
      "(53, -10.186186700158263)\n",
      "(62, -9.8718297227162477)\n",
      "(57, -9.3086742424946145)\n",
      "(66, -8.664186773794567)\n",
      "(71, -8.6337776889025477)\n",
      "(52, -8.5745084898295811)\n",
      "(61, -8.2442173592336072)\n",
      "(56, -7.7144025914315231)\n",
      "(70, -7.1042873999022209)\n",
      "(65, -7.0953924257124914)\n",
      "(51, -7.0279179715871489)\n",
      "(60, -6.7068550075407645)\n",
      "(55, -6.2182626140081156)\n",
      "(50, -5.5891383854277921)\n",
      "(44, -2.9139390319956764)\n",
      "(39, -2.8891310758503432)\n",
      "(34, -2.8577848351118202)\n",
      "(29, -2.8170591933768381)\n",
      "(480, -2.800645855863622)\n",
      "(49, -2.6870377962244745)\n",
      "(43, -2.5853973837144002)\n",
      "(38, -2.5692651890759666)\n",
      "(33, -2.5484124528975358)\n",
      "(481, -2.5273452097683244)\n",
      "(28, -2.5213424620904927)\n",
      "(48, -2.3672764494201219)\n",
      "(42, -2.2771267938883883)\n",
      "(37, -2.2688187834626845)\n",
      "(32, -2.2578272327536224)\n",
      "(482, -2.2431797097416211)\n",
      "(27, -2.2425740412373831)\n",
      "(47, -2.068737826053892)\n",
      "(41, -1.9902213104474515)\n",
      "(36, -1.988177287268202)\n",
      "(31, -1.9854003516297183)\n",
      "(26, -1.9807933721964819)\n",
      "(483, -1.9569597424889515)\n",
      "(46, -1.7921243612714644)\n",
      "(25, -1.736901577227997)\n",
      "(30, -1.7324108855427052)\n",
      "(35, -1.728281243021597)\n",
      "(40, -1.7242727427369091)\n",
      "(484, -1.6780993648954419)\n",
      "(45, -1.5376370293866766)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAqegFxEpcAp6EZEC9/8BfNYedBA/27sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183ebc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif, f_regression\n",
    "\n",
    "#########SELECTKBEST################\n",
    "\n",
    "%matplotlib inline  \n",
    "selector = SelectKBest(f_regression, k=\"all\")\n",
    "selector.fit(Xtrain, ytrain)\n",
    "sorted_attributes = np.argsort(-selector.scores_)\n",
    "sorted_scores = np.sort(-selector.scores_)\n",
    "for index,element in enumerate(zip(sorted_attributes, sorted_scores)):\n",
    "    print element\n",
    "    if index>550: break\n",
    "        \n",
    "plt.plot(-sorted_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 75,  76,  80,  85,  81,  77,  90,  86,  82,  78,  91,  87,  95,\n",
       "        83,  92,  79,  96,  88,  84,  93,  97,  89,  98,  94,  99, 419,\n",
       "       414, 409, 404, 424, 418, 413, 408, 403, 423, 417, 412, 407, 402,\n",
       "       422, 416, 411, 406, 401, 421, 199, 194, 415, 189, 410, 405, 400,\n",
       "       420, 184, 379, 384, 389, 394, 179, 495, 399, 198, 193, 496, 188,\n",
       "       299, 497, 183, 294, 549, 544, 378, 289, 383, 388, 393, 539, 178,\n",
       "       284, 534, 398, 197, 498, 529, 192, 279, 187, 298, 293, 548, 543,\n",
       "       470, 182, 288, 538, 377, 533, 382, 283, 387, 392, 499, 177, 528,\n",
       "       196, 278, 397, 450, 191, 455, 297, 460, 465, 547, 186, 292, 542,\n",
       "       537, 287, 181, 532, 282, 527, 376, 176, 381, 277, 386, 391, 195,\n",
       "       296, 546, 190, 541, 396, 291, 536, 185, 286, 531, 281, 180, 526,\n",
       "       276, 471, 545, 175, 295, 540, 375, 290, 535, 380, 385, 390, 285,\n",
       "       530, 451, 395, 456, 280, 461, 525, 466, 275,   0,   1,   2,   3,\n",
       "         5,   4,   6,   7,  10,   8,  11,   9,  12,  15,  13,  16,  14,\n",
       "        17,  18,  19,  20,  21,  22,  23,  24, 490, 491, 492, 472, 452,\n",
       "       457, 462, 493, 467, 494, 473, 453, 458, 463, 468, 259, 254, 264,\n",
       "       269, 258, 263, 268, 253, 262, 267, 257, 504, 252, 266, 261, 256,\n",
       "       274, 273, 272, 251, 265, 503, 260, 271, 255, 509, 250, 270, 502,\n",
       "       508, 159, 154, 164, 514, 158, 169, 163, 153, 501, 168, 507, 513,\n",
       "       157, 162, 167, 519, 152, 161, 156, 500, 166, 506, 174, 512, 518,\n",
       "       173, 151, 172, 160, 165, 155, 511, 505, 517, 171, 150, 524, 170,\n",
       "       523, 516, 510, 522, 515, 521, 520, 200, 201, 202, 203, 204, 206,\n",
       "       207, 205, 208, 209, 212, 213, 211, 214, 210, 218, 217, 219, 216,\n",
       "       215, 229, 234, 228, 239, 249, 233, 227, 244, 238, 248, 232, 226,\n",
       "       243, 223, 224, 222, 221, 220, 237, 247, 231, 242, 225, 236, 246,\n",
       "       230, 241, 235, 245, 149, 134, 139, 129, 144, 240, 148, 138, 128,\n",
       "       143, 133, 147, 127, 132, 142, 137, 146, 136, 141, 126, 131, 474,\n",
       "       145, 125, 140, 135, 130, 454, 124, 459, 119, 109, 104, 114, 329,\n",
       "       464, 334, 469, 123, 349, 339, 108, 103, 118, 113, 328, 344, 333,\n",
       "       122, 348, 338, 112, 107, 102, 117, 327, 343, 332, 121, 337, 347,\n",
       "       111, 101, 116, 106, 342, 326, 331, 120, 336, 346, 115, 110, 100,\n",
       "       105, 341, 325, 330, 335, 345, 340, 354, 359, 353, 374, 364, 358,\n",
       "       369, 352, 373, 363, 357, 368, 351, 372, 362, 367, 356, 350, 371,\n",
       "       361, 366, 355, 370, 360, 365, 324, 304, 309, 314, 319, 303, 323,\n",
       "       308, 313, 318, 302, 322, 307, 312, 317, 301, 306, 321, 311, 316,\n",
       "       300, 305, 320, 310, 315, 485, 486, 487, 488, 489, 479, 478, 477,\n",
       "       476, 475, 425, 426, 427, 428, 429, 430, 431, 432, 433, 435, 434,\n",
       "       436, 437, 440, 438, 445, 441, 439, 446, 442, 447, 443, 444, 448,\n",
       "       449,  69,  74,  64,  59,  68,  73,  54,  63,  58,  67,  72,  53,\n",
       "        62,  57,  66,  71,  52,  61,  56,  70,  65,  51,  60,  55,  50,\n",
       "        44,  39,  34,  29, 480,  49,  43,  38,  33, 481,  28,  48,  42,\n",
       "        37,  32, 482,  27,  47,  41,  36,  31,  26, 483,  46,  25,  30,\n",
       "        35,  40, 484,  45], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new sorted data\n",
    "XtrainNew = Xtrain[:, sorted_attributes]\n",
    "XtestNew = Xtest[:, sorted_attributes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Tree with all attributes is 384.809654028\n"
     ]
    }
   ],
   "source": [
    "#Original Regression Tree\n",
    "t1 = tree.DecisionTreeRegressor(random_state=0)\n",
    "t1 = t1.fit(XtrainNew, ytrain)\n",
    "ytestPred = t1.predict(XtestNew)\n",
    "print \"MAE for Tree with all attributes is {}\".format(mae(ytestPred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Tree with 400 attributes is 388.124018957\n"
     ]
    }
   ],
   "source": [
    "#Regression Tree with 400 attributes\n",
    "t2 = tree.DecisionTreeRegressor(random_state=0)\n",
    "t2 = t2.fit(XtrainNew[:,:400], ytrain)\n",
    "ytestPred2 = t2.predict(XtestNew[:,:400])\n",
    "print \"MAE for Tree with 400 attributes is {}\".format(mae(ytestPred2, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Tree with 200 attributes is 400.66792891\n"
     ]
    }
   ],
   "source": [
    "##Regression Tree with 200 attributes\n",
    "t3 = tree.DecisionTreeRegressor(random_state=0)\n",
    "t3 = t2.fit(XtrainNew[:,:200], ytrain)\n",
    "ytestPred3 = t3.predict(XtestNew[:,:200])\n",
    "print \"MAE for Tree with 200 attributes is {}\".format(mae(ytestPred3, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Tree with 70 attributes is 390.07336019\n"
     ]
    }
   ],
   "source": [
    "#Regression Tree with 70 attributes\n",
    "t4 = tree.DecisionTreeRegressor(random_state=0)\n",
    "t4 = t4.fit(XtrainNew[:,:70], ytrain)\n",
    "ytestPred4 = t4.predict(XtestNew[:,:70])\n",
    "print \"MAE for Tree with 70 attributes is {}\".format(mae(ytestPred4, ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can see that by selecting the number of attribute with SelectKBest the error is not increasing in a significative way: \n",
    "#it grows from the value of 383.927113744 with the 550 attributes to 390.07336019 with only 70 of them. Approximately the same \n",
    "#value (388.124018957) is reached with 400 attributes (but is selecting less), while with 200 attributes the error is even higher going up to \n",
    "#400.66792891."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 550 candidates, totalling 2750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2750 out of 2750 | elapsed: 30.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 38s\n"
     ]
    }
   ],
   "source": [
    "###########PIPELINE##########\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {'feature_selection__k': np.arange(Xtrain.shape[1])+1}\n",
    "\n",
    "treepipe = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_regression)),\n",
    "  ('regression', tree.DecisionTreeRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "np.random.seed(0)\n",
    "treepipe_grid = GridSearchCV(treepipe, \n",
    "                        param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=5 , n_jobs=1, verbose=1)\n",
    "%time _ = treepipe_grid.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Tree found with Pipeline is 372.161416474 and the selected attributes are {'feature_selection__k': 521}\n"
     ]
    }
   ],
   "source": [
    "ytestPredPipe = treepipe_grid.predict(Xval)\n",
    "print \"MAE for Tree found with Pipeline is {} and the selected attributes are {}\".format(mae(ytestPredPipe, yval),treepipe_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MAE is close to the previous, however the number of preditors haven't decrease that much (from 550 to 521). Maybe by adding\n",
    "#other parameters such as 'regression__min_samples_split' the algorithm will sift through the attributes in a deeper way, but \n",
    "#fitting so many folds to each of the candidates really took too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2528L, 22L)\n",
      "(1299L, 22L)\n",
      "(2110L, 22L)\n"
     ]
    }
   ],
   "source": [
    "#FOCUSING ON SOTAVENTO\n",
    "\n",
    "Xava = ava[:,6:]; yava = ava[:,0]\n",
    "Xtrain = ava[indicesTrain,6:]; ytrain = ava[indicesTrain,0]\n",
    "Xval = ava[indicesVal,6:]; yval = ava[indicesVal,0]\n",
    "Xtest = ava[indicesTest,6:]; ytest = ava[indicesTest,0]\n",
    "\n",
    "import numpy\n",
    "XtrainSot=numpy.zeros(shape=(2528,22))\n",
    "for i in range (0,21):\n",
    "    XtrainSot[:,i]=ava[indicesTrain,(i*25)+18]\n",
    "\n",
    "print(XtrainSot.shape)\n",
    "\n",
    "XvalSot=numpy.zeros(shape=(1299,22))\n",
    "for i in range (0,21):\n",
    "    XvalSot[:,i]=ava[indicesVal,(i*25)+18]\n",
    "\n",
    "print(XvalSot.shape)\n",
    "\n",
    "XtestSot=numpy.zeros(shape=(2110,22))\n",
    "for i in range (0,21):\n",
    "    XtestSot[:,i]=ava[indicesTest,(i*25)+18]\n",
    "\n",
    "print(XtestSot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for RandomForest with default hyper parameters is 283.218431521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfSot = RandomForestRegressor(n_estimators=55)\n",
    "rfSot.fit(XtrainSot, ytrain)\n",
    "\n",
    "yval_predrfSot = rfSot.predict(XvalSot)\n",
    "print \"MAE for RandomForest with default hyper parameters is {}\".format(mae(yval_predrfSot, yval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for RandomForest with default hyper parameters is 293.898268763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfSot = RandomForestRegressor(n_estimators=55)\n",
    "rfSot.fit(XtrainSot, ytrain)\n",
    "\n",
    "yval_predrfSot2 = rfSot.predict(XtestSot)\n",
    "print \"MAE for RandomForest with default hyper parameters is {}\".format(mae(yval_predrfSot2, ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#both errors on the validation and test sets results to be a little bit higher than the ones with all 550 attributes, but the\n",
    "#performance with the subset of attributes of Sotavento is basically the same as the previous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
